{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c18321f7",
   "metadata": {},
   "source": [
    "# 层和块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d075137e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-13T05:47:43.235688Z",
     "start_time": "2022-10-13T05:47:43.229982Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c97785c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-13T05:51:51.114983Z",
     "start_time": "2022-10-13T05:51:51.025016Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1206,  0.0104,  0.1751, -0.1819, -0.0941,  0.1724, -0.1737,  0.2219,\n",
       "          0.1153, -0.0852],\n",
       "        [ 0.0916,  0.1644,  0.1013, -0.1384, -0.0599, -0.0059, -0.1495,  0.2479,\n",
       "          0.1370, -0.0191]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 20 输入特征，256线性层之后的输出特征；输出特征也是下一层的输入特征数；\n",
    "net = nn.Sequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))\n",
    "\n",
    "X = torch.rand(2, 20)\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5cc432",
   "metadata": {},
   "source": [
    "## 自定义块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dece1fc5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-13T05:54:42.871232Z",
     "start_time": "2022-10-13T05:54:42.867041Z"
    }
   },
   "outputs": [],
   "source": [
    "# 自定义一个MLP；\n",
    "class MLP(nn.Module):\n",
    "    # 用模型参数声明层。这里，我们声明两个全连接的层\n",
    "    def __init__(self):\n",
    "        # 调用MLP的父类Module的构造函数来执行必要的初始化。\n",
    "        # 这样，在类实例化时也可以指定其他函数参数，例如模型参数params（稍后将介绍）\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(20, 256)  # 隐藏层\n",
    "        self.out = nn.Linear(256, 10)  # 输出层\n",
    "\n",
    "    # 定义模型的前向传播，即如何根据输入X返回所需的模型输出\n",
    "    def forward(self, X):\n",
    "        # 注意，这里我们使用ReLU的函数版本，其在nn.functional模块中定义。\n",
    "        return self.out(F.relu(self.hidden(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e119aae2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-13T05:54:57.365226Z",
     "start_time": "2022-10-13T05:54:57.354941Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4841,  0.0017,  0.3030,  0.0601,  0.3415, -0.0062, -0.1639,  0.1265,\n",
       "         -0.1160,  0.3818],\n",
       "        [-0.4571,  0.0191,  0.1765,  0.0348,  0.2829,  0.0665, -0.1169,  0.2575,\n",
       "          0.0092,  0.3055]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3e9f49",
   "metadata": {},
   "source": [
    "## 顺序块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4dad77c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-13T06:10:29.594896Z",
     "start_time": "2022-10-13T06:10:29.579194Z"
    }
   },
   "outputs": [],
   "source": [
    "class MySequential(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        for idx, module in enumerate(args):\n",
    "            # 这里，module是Module子类的一个实例。我们把它保存在'Module'类的成员\n",
    "            # 变量_modules中。_module的类型是OrderedDict\n",
    "            self._modules[str(idx)] = module\n",
    "            print(str(idx))\n",
    "\n",
    "    def forward(self, X):\n",
    "        # OrderedDict保证了按照成员添加的顺序遍历它们\n",
    "        for block in self._modules.values():\n",
    "            X = block(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e58f252e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-13T06:13:13.925578Z",
     "start_time": "2022-10-13T06:13:13.812148Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0341, -0.1112,  0.0612,  0.1057, -0.3129,  0.1261, -0.0573,  0.0412,\n",
       "         -0.0237,  0.0511],\n",
       "        [ 0.0534, -0.0369,  0.1187,  0.0232, -0.3427,  0.1436, -0.1232, -0.0043,\n",
       "         -0.0775,  0.1310]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MySequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a909435",
   "metadata": {},
   "source": [
    "## 在前向传播函数中执行代码\n",
    "\n",
    "继承了nn.Module类，可以很灵活的实习各种想法；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "063eb4c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-13T06:21:15.024533Z",
     "start_time": "2022-10-13T06:21:14.986258Z"
    }
   },
   "outputs": [],
   "source": [
    "class FixedHiddenMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 不计算梯度的随机权重参数。因此其在训练期间保持不变\n",
    "        self.rand_weight = torch.rand((20, 20), requires_grad=False)\n",
    "        self.linear = nn.Linear(20, 20)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.linear(X)\n",
    "        # 使用创建的常量参数以及relu和mm函数\n",
    "        X = F.relu(torch.mm(X, self.rand_weight) + 1)\n",
    "        # 复用全连接层。这相当于两个全连接层共享参数\n",
    "        X = self.linear(X)\n",
    "        # 控制流\n",
    "        while X.abs().sum() > 1:\n",
    "            X /= 2\n",
    "        return X.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83416643",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-13T06:21:19.843628Z",
     "start_time": "2022-10-13T06:21:19.795264Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.2018, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = FixedHiddenMLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29f5a45",
   "metadata": {},
   "source": [
    "我们可以混合搭配各种组合块的方法。 在下面的例子中，我们以一些想到的方法嵌套块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d43f81c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-13T06:21:57.304637Z",
     "start_time": "2022-10-13T06:21:57.289184Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0500, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NestMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(20, 64), nn.ReLU(),\n",
    "                                 nn.Linear(64, 32), nn.ReLU())\n",
    "        self.linear = nn.Linear(32, 16)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.linear(self.net(X))\n",
    "\n",
    "chimera = nn.Sequential(NestMLP(), nn.Linear(16, 20), FixedHiddenMLP())\n",
    "chimera(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5178fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
